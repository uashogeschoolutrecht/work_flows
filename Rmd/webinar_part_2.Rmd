---
title: "Part 2; Guerilla Analytics"
author: "Marc A.T. Teunis"
date: "6/10/2020"
output:
  ioslides_presentation:
    highlight: tango
subtitle: Why, What and How in a series of three webinars
always_allow_html: yes
widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  error = FALSE, 
  fig.width = 6,
  fig.height = 4)

image_dir <- here::here(
  "images"
)

## packages
library(tidyverse)
library(readxl)

```
# Webinar 1; Open Science introduction

## Example; The Open Science Framework [OSF](https://osf.io/)
```{r}
knitr::include_graphics(
  here::here(
    "images",
    "cos-shield.png")
)
```

$Reproducible\ Science = P + D + C + OAcc + OSrc$ 

**OSF has it all**

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 

## OSF - Reproducible Project: Psychology

 >- 100 publications in Psychology journals
 >- Results from half of these publications could be reproduced
 >- Full access to P, D and C in [OSF](https://osf.io/ezcuj/)
 >- The publication is not published in an OAcc journal but:
 >- [The submitted manuscript is available in OSF](http://pps.sagepub.com/content/7/6/657.abstract)
 >- [The R code used is available in OSF](https://osf.io/fkmwg/)  
 
 $RP:Psychology = P + D + C + OSrc\ (+ OAcc)$

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 


# Part 2; Managing your project with 'Guerilla Analytics'

 >1. Guerilla Analytics principles
 >1. Files and folders / Project structure
 >1. Data integrity (md5sums)
 >1. Data-formats
 >1. Data shapes / Tidy data
 >1. Encoding variables
 
## Do you recognize this!

<p style="font-size:14px">from: https://medium.com/@jameshoareid/final-pdf-finalfinal-pdf-actualfinal-pdf-cae61ab1d94c</p>
```{r, dpi=50}
knitr::include_graphics(
  file.path(
  image_dir,
    "final_final.png"
  )
)

```


## The Guerilla Analytics Principles

 >1: Space is cheap, confusion is expensive
 >2: Prefer simple, visual project structures and conventions
 >3: Prefer automation with program code
 >4: Maintain a link between data on the file system, data in the analytics environment, and data in work products
 >5: Version control changes to data and analytics code 
 >6: Consolidate team knowledge in version-controlled builds 
 >7: Prefer analytics code that runs from start to finish

<p style="font-size:14px">[Guerilla Analytics book](https://guerrilla-analytics.net/)</p>

## P1: Space is cheap, confusion is expensive

 - Keep your files, you never know when you need them
 - Store files in online-cloud storage (HU Research Drive) and make regular offline backups (if applicable)
 - Protect youself: do not click on attachments and spiffy emails, cybercriminals are getting smarter everyday
 - Create md5sums for important (source) data-files
 - Agree on a system, use it, stick to it
 
```{r, results='markup'}
knitr::include_graphics(
  file.path(
    image_dir,
    "docking_station_harddisk.jpg"
  )
)
```

## P2: Simple, visual project structures and conventions    

 >- Create a seperate folder for each project (in RStudio -> RStudio Project)
 >- Do not nest folders deeply (max 2-3 levels)
 >- Keep information about the data, close to the data
 >- Store each dataset in its own subfolder
 >- Do not change file names or move them (in a code project)
 >- Do not manually edit data source files
 >- In code, use relative paths
 
## Better not!
```{r}
fs::dir_tree(here::here("wrong_structure"))
```

## How to organize data files
```{r}
fs::dir_tree(here::here("data-raw"))
```

## Be relative!
[Use the `{here}` package!](https://github.com/jennybc/here_here) My slides are `here`
```{r, echo=TRUE}
## relative path 
basename(path = here::here("Rmd", "webinar_part_2.html"))
## the working directory (never set it manually!!)
here::here()
## the actual path
basename(path = "D:/r_projects/workflows/Rmd/webinar_part_2.html")
```

## Data integrity

MD5SUMS are

 >- A unique code to identify a file
 >- Can be used to verify the integrity and the version of a file
 >- Can be genarated from Windows, MacOS, Linux or from within e.g. R/Python/Bash
 >- Share an md5sum with your (shared) data
 >- If you sent a data file, sent the md5sum along
 >- There are a number of hashes md5 sums are much used in Open Source communities
 >- md5sums are also used for safety: checking an md5sum ensures that the code is valid and has not changed
 
## How does that look?

```{r, echo=TRUE}
head(readr::read_csv(here::here("data", "D020", "tidy_excel.csv")), 2)
readr::read_lines(here::here("data", "D020", "tidy_excel.csv.md5"))
tools::md5sum(here::here("data", "D020", "tidy_excel.csv"))

```
 
## Sharing data

 >- Remove sensitive data from each file by pseudoencoding or anonymizing
 >- Removing sensitive data can be done from within R (I will show you in webinar 3)
 >- Share md5sums of files together so the reciever can check the the transfer and the version of the file
 >- Agree on a file naming convention within a team, before the work starts
 >- Agree on where data is stored and who has access
 >- Suppress the impulse to store multiple copies of the data in different locations

## Recieving data

 >- Never change a filename (as inconvenient it may be)
 >- Put a new dataset (one or multiple files) in its own numbered folder
 >- Write an README.txt describing the data, store it in the same folder as where the dataset lives
 >- A new version of the 'same' dataset goes into the orginal folder, the old version moves to a new folder (e.g v01)

## The next Guerilla Analytics principle requires an example
```{r}
fs::dir_tree(here::here(
  "data-raw",
  "D010"
  
))
```

## P3: Automation

 >- Do everything programatically (in code) for reasons of reproducibility
 >- Store clean curated datasets in the "data" folder, with md5sums and a README.txt
 >- Use literate programming (e.g LaTeX or RMarkdown) for full analysis
 >- Store scripts in a "./code" or "./inst" folder
 >- Store functions in R in a "./R" folder
 
## A COVID-19 example

 >- Imagine we want daily reports on the number of COVID-19 cases and caused deaths
 >- We want to be able to dynamically report data for different countries and dates to compare situations in the World
 >- The data is available (for manual and automated download) from the European Center for Disease Control
 >- The analysis can be coded completely from begin to end to result in the information we need
 
## The results of the analysis; deaths
```{r, fig.width=8, fig.height=5}
source(here::here("code", "covid_cases.R")); plot
```

## The results of the analysis; cases
```{r, fig.width=8, fig.height=5}
source(here::here("code", "covid_cases.R")); plot_cases
```
 
## Let's take a look at the source file

The source file is an RMarkdown file that downloads the data and generates an HTML report including two figures.

```{r, dpi = 60}
knitr::include_graphics(here::here(
    "images",
    "covid_rmd_screenshot.png"
  )
)
```

## Parameterization

 >- This Rmd is parameterized on date and country
 >- The script automatically includes the parameters in the title of the report and the captions of the figures
 >- The 'rendered' date is automatically set, for versioning
 >- Parameterization can used to automate reporting for many values of parameters 

## Further automation is easy now

Assume we want to create a report from the ECDC COVID-data on a daily basis.

We could render the Rmd file manually, every day ...

Better: Write an small R script that renders the report daily and stores it (with a date-stamp in the file name) in the right folder.

The script is included in the repository where the source code 
for these slides are stored. It can be found here:

```{r}
fs::dir_tree(
  here::here(
  "code"
  )
)
## the daily reports are here:
fs::dir_tree(
  here::here(
  "Rmd",
  "automated_reports"
  )
)

```

## P4: Maintain a link between data on the file system, data in the analytics environment, and data in work products

Basically this can be covered when you work with R or Python in RStudio or Jupyter:

 - The file system is the data (stored on disk or elsewhere)
 - The analytics environment is the Global Environment (where variables and R-obejcts live)
 - The work products can be Rmd files (or Jupyter Notbooks) that bring it all together

```{r, dpi=200, fig.align='right'}
knitr::include_graphics(
  file.path(
  image_dir,
  "one_ring.jpg")
)
```

## Output formats

Using RMarkdown (in stead of Word, Excell, Powerpoint or in their combination) enables you to

 - Link data to the analysis code and to the narratives
 - Use the same source for different types of outputs (see folder "./Rmd" in the repo - covid example)
 - Use other markup languages like HTML, CSS and LaTeX together with R
 - Dynamically write the same reports on different source data 

## P5: Version control for data and code - Git/Github

 - When you write code, you should use Git, preferably in combination with Github
 - Track changes for code
 
Imagine working on a script together with a colleague in Groningen. You email your code, and your data, she makes adjustments and sents the code back to you, you test the code and change something, the code breaks...you are lost on what she changed and what you changed...

Git/Github is the solution:
[Tutorial Git/Github and RStudio](https://happygitwithr.com/)

_"Learning Git can be challenging at first, but it pays off in the long run. Eventually you will always break working code, multiple times"_

## The github repo where these slides live

https://github.com/uashogeschoolutrecht/work_flows

Getting them is easy (if you are not afraid of the Terminal)

Windows -> cmd -> cd to dir -> `git clone https://github.com/uashogeschoolutrecht/work_flows`  

## A normal git workflow; segregating data from compute infrastructure from code

```{r, dpi = 70}
knitr::include_graphics(
  file.path(image_dir,
            "git_workflow.jpg")
)
```

_We will get our hands dirty on this in Webinar 3_

## P6: Consolidate team knowledge in version-controlled builds 

 - Make guidelines on datamanagement, storage places and workflows
 - Agree within the team on them
 - Stick to them
 - Work together in a virtual collaboration envrionment (VRE)
 - Work together on code using Github
 - Provide for education and share best practices within the organization, the department and/or the team

## P7: Prefer analytics code that runs from start to finish

 - Create workproducts in RMarkdown or Jupyter notebooks (I will show in Webinar 3 what that is about)

# Some directives that you could start using today

## File names and file formats

 - Never use `!@#$%^&*()+=:;"'|{}[]\<>?/~` in a file name
 - Use `snake_case` or `CamelCase` instead
 - Apply this also to file headers (column names)
 - Do not use soft spaces (`" "` = soft space / `"_"` = hard space)

## A how-not-to example

```{r, dpi = 80}
knitr::include_graphics(
  file.path(image_dir, "bad_formatting_file_name_and_headers.jpg")
)
```

_what is wrong with this file name and its haders? can you spot another problem with the data sheet?_

## Data-formats - Proprietary

File format source code is protected under patent or IP, source is not available, maintained by a company [although .xlsx is an implementation of xml]

  - .xls (Excel Legacy)  
  - .xlsx (Excel)
  - .sav (SPSS) 
  - .pzm (GraphPad)  
  - .docx (MS Office Word) 
  - .pdf (Adobe)

_Can be changed by company, without backwards compatility_

## Data-formats - Non-Proprietary

File format source code is open source source, maintained by open source community or core development team

  - .netCDF (Geo, proteomics, array-oriented scientific data)  
  - .xml / mzXML (Markup language, human and machine readable, metadata + data together)
  - .txt / .csv (flat text file, usually tab-comma or semi colon seperated) 
  - .json (text format that is completely language independent)  

_Will remain readable, even if format becomes obsolete_

## Data shape
Look at these two tables, what do you notice?
```{r}
table2
table3
```

<p style="font-size:14px">Both tables are build-in datasets from the {tidyr} package belonging to the {tidyverse} suite of Data Science R packages</p> 

## Tidy data
```{r}
knitr::include_graphics(
  file.path(image_dir, "tidy-1.png")
)
```

 1. Each variable goes in its own column
 1. Each observation goes in its own row
 1. Each cell contains only one value

<p style="font-size:14px">From: ["R for Data Science", Grolemund and Wickham](https://r4ds.had.co.nz/)</p> 

## Are these tidy? {build}
```{r}
map(
  .x = list(table5,
table4a),
.f = head, 3)
```

_What steps would we need to tidy them?_ 










```{r}
# install.packages("remotes")
# remotes::install_github("allisonhorst/palmerpenguins")
library(palmerpenguins)
data(package = "palmerpenguins")

```





## Variable encodings
 


## Meta data
http://rd-alliance.github.io/metadata-directory/standards/


## Thank you for your attention!

```{r, dpi = 150}
knitr::include_graphics(
  file.path(
    image_dir,
    "Pepper.png")
)
```

**UPCOMING WEBINARS:**

Part 3; Reproducible (Open) Science @HU - Tools (July 6th, 2020)
 
[Peer Support Group Data Science](tln.hu.nl)
[support voor onderzoek](https://bibliotheek.hu.nl/onderzoekers/)


